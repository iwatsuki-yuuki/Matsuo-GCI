{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwatsuki-yuuki/Matsuo-GCI/blob/main/DL%E5%9F%BA%E7%A4%8E%E8%AC%9B%E5%BA%A7%E7%AC%AC%EF%BC%97%E5%9B%9E%E8%AA%B2%E9%A1%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI0CCRYGbsLX"
      },
      "source": [
        "# 第7回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhZjl_2gbsLf"
      },
      "source": [
        "### 課題\n",
        "RNNを用いてIMDbのsentiment analysisを実装してみましょう．\n",
        "\n",
        "ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBcfs2VybsLj"
      },
      "source": [
        "### 目標値\n",
        "F値：0.85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZj-U7SvbsLl"
      },
      "source": [
        "### ルール\n",
        "\n",
        "- **以下のセルで指定されている`x_train`, `t_train`以外の学習データは使わないでください．**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm9ZJX2TbsLo"
      },
      "source": [
        "### 提出方法\n",
        "- 2つのファイルを提出していただきます．\n",
        "  1. テストデータ `x_test` に対する予測ラベルを`submission_pred.csv`として保存し，Omnicampusの宿題から**「第7回 回帰結合型ニューラルネットワーク」を選択して**提出してください．\n",
        "  2. それに対応するpythonのコードを`submission_code.py`として保存し，**Omnicampusの宿題から「第7回 回帰結合型ニューラルネットワーク (code)」を選択して**提出してください．提出してください．pythonファイル自体の提出ではなく，「提出内容」の部分にコードをコピー&ペーストしてください．\n",
        "\n",
        "- なお，採点は1で行い，2はコードの確認用として利用します（成績優秀者はコード内容を公開させていただくかもしれません）．コードの内容を変更した場合は，**1と2の両方を提出し直してください**．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejdA6CESbsLs"
      },
      "source": [
        "### 評価方法\n",
        "\n",
        "- 予測ラベルの`t_test`に対するF値で評価します．\n",
        "- 即時採点しLeader Boardを更新します．（採点スケジュールは別アナウンス）\n",
        "- 締切時の点数を最終的な評価とします．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyYaymAQ4Wey"
      },
      "source": [
        "### ドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Szesnftz4a1K",
        "outputId": "ae7389b3-8dce-445f-dd0c-da89fc32f428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 作業ディレクトリを指定\n",
        "work_dir = 'drive/MyDrive/Colab Notebooks/DLBasics2025_colab'"
      ],
      "metadata": {
        "id": "jeR6ST382WcT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjSgBf1ibsLw"
      },
      "source": [
        "### データの読み込み（このセルは修正しないでください）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4X9ptijybsLz",
        "outputId": "19737498-012f-432a-e273-41ddeb6f5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n",
            "単語種数: 88587\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "# 学習データ\n",
        "x_train = np.load(work_dir + '/Lecture07/data/x_train.npy', allow_pickle=True)\n",
        "t_train = np.load(work_dir + '/Lecture07/data/t_train.npy', allow_pickle=True)\n",
        "\n",
        "# 検証データを取る\n",
        "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=seed)\n",
        "\n",
        "# テストデータ\n",
        "x_test = np.load(work_dir + '/Lecture07/data/x_test.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "def text_transform(text: List[int], max_length=256):\n",
        "    # <BOS>はすでに1で入っている．<EOS>は2とする．\n",
        "    text = text[:max_length - 1] + [2]\n",
        "\n",
        "    return text, len(text)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, len_seq_list = [], [], []\n",
        "\n",
        "    for sample in batch:\n",
        "        if isinstance(sample, tuple):\n",
        "            label, text = sample\n",
        "\n",
        "            label_list.append(label)\n",
        "        else:\n",
        "            text = sample.copy()\n",
        "\n",
        "        text, len_seq = text_transform(text)\n",
        "        text_list.append(torch.tensor(text))\n",
        "        len_seq_list.append(len_seq)\n",
        "\n",
        "    # NOTE: 宿題用データセットでは<PAD>は3です．\n",
        "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3).T, torch.tensor(len_seq_list)\n",
        "\n",
        "\n",
        "word_num = np.concatenate(np.concatenate((x_train, x_test))).max() + 1\n",
        "print(f\"単語種数: {word_num}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKgr01wbsL5"
      },
      "source": [
        "### 実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WPhmbQtagOvH"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_train, x_train)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    [(t, x) for t, x in zip(t_valid, x_valid)],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    x_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w6j2soNLbsL7"
      },
      "outputs": [],
      "source": [
        "def torch_log(x):\n",
        "    return torch.log(torch.clamp(x, min=1e-10))\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        glorot = 6/(in_dim + hid_dim*2)\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_i = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_f = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_o = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "        self.W_c = nn.Parameter(torch.tensor(np.random.uniform(\n",
        "                        low=-np.sqrt(glorot),\n",
        "                        high=np.sqrt(glorot),\n",
        "                        size=(in_dim + hid_dim, hid_dim)\n",
        "                    ).astype('float32')))\n",
        "        self.b_c = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
        "\n",
        "    def function(self, state_c, state_h, x):\n",
        "        i = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_i) + self.b_i)\n",
        "        f = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_f) + self.b_f)\n",
        "        o = torch.sigmoid(torch.matmul(torch.cat([state_h, x], dim=1), self.W_o) + self.b_o)\n",
        "        c = f*state_c + i*torch.tanh(torch.matmul(torch.cat([state_h, x], dim=1), self.W_c) + self.b_c)\n",
        "        h = o*torch.tanh(c)\n",
        "        return c, h\n",
        "\n",
        "    def forward(self, x, len_seq_max=0, init_state_c=None, init_state_h=None):\n",
        "        x = x.transpose(0, 1)\n",
        "        state_c = init_state_c\n",
        "        state_h = init_state_h\n",
        "        if init_state_c is None:  # 初期値を設定しない場合は0で初期化する\n",
        "            state_c = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
        "        if init_state_h is None:  # 初期値を設定しない場合は0で初期化する\n",
        "            state_h = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
        "\n",
        "        size = list(state_h.unsqueeze(0).size())\n",
        "        size[0] = 0\n",
        "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
        "\n",
        "        if len_seq_max == 0:\n",
        "            len_seq_max = x.size(0)\n",
        "        for i in range(len_seq_max):\n",
        "            state_c, state_h = self.function(state_c, state_h, x[i])\n",
        "            output = torch.cat([output, state_h.unsqueeze(0)])  # 出力系列の追加\n",
        "        return output\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb = nn.Embedding(word_num, emb_dim, padding_idx=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x)\n",
        "\n",
        "class SequenceTaggingNet(nn.Module):\n",
        "    def __init__(self, word_num, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.emb = Embedding(word_num, emb_dim)\n",
        "        self.lstm = LSTM(emb_dim, hid_dim)\n",
        "        self.linear = nn.Linear(hid_dim, 1)\n",
        "\n",
        "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
        "        h = self.emb(x)\n",
        "        h = self.lstm(h, len_seq_max, init_state)\n",
        "        if len_seq is not None:\n",
        "            # 系列が終わった時点での出力を取る必要があるので len_seq を元に集約する\n",
        "            h = h[len_seq - 1, list(range(len(x))), :]\n",
        "        else:\n",
        "            h = h[-1]\n",
        "\n",
        "        y = self.linear(h)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9w3yjqqCl_Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d71d44-ba7c-4ca2-d690-78f4b3f2d775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0, Train Loss: 0.641, Valid Loss: 0.541, Validation F1: 0.740\n",
            "EPOCH: 1, Train Loss: 0.471, Valid Loss: 0.464, Validation F1: 0.794\n",
            "EPOCH: 2, Train Loss: 0.367, Valid Loss: 0.443, Validation F1: 0.827\n",
            "EPOCH: 3, Train Loss: 0.295, Valid Loss: 0.404, Validation F1: 0.829\n",
            "EPOCH: 4, Train Loss: 0.237, Valid Loss: 0.401, Validation F1: 0.835\n",
            "EPOCH: 5, Train Loss: 0.197, Valid Loss: 0.453, Validation F1: 0.834\n",
            "EPOCH: 6, Train Loss: 0.159, Valid Loss: 0.494, Validation F1: 0.845\n",
            "EPOCH: 7, Train Loss: 0.132, Valid Loss: 0.457, Validation F1: 0.844\n",
            "EPOCH: 8, Train Loss: 0.103, Valid Loss: 0.533, Validation F1: 0.849\n",
            "EPOCH: 9, Train Loss: 0.085, Valid Loss: 0.520, Validation F1: 0.848\n"
          ]
        }
      ],
      "source": [
        "emb_dim = 100\n",
        "hid_dim = 50\n",
        "n_epochs = 10\n",
        "device = 'cuda'\n",
        "\n",
        "net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
        "net.to(device)\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "\n",
        "    net.train()\n",
        "    n_train = 0\n",
        "    acc_train = 0\n",
        "    for label, line, len_seq in train_dataloader:\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "        t = label.to(device)\n",
        "        x = line.to(device)\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "        loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        losses_train.append(loss.tolist())\n",
        "\n",
        "        n_train += t.size()[0]\n",
        "\n",
        "    t_valid = []\n",
        "    y_pred = []\n",
        "    net.eval()\n",
        "    for label, line, len_seq in valid_dataloader:\n",
        "\n",
        "        t = label.to(device)\n",
        "        x = line.to(device)\n",
        "        len_seq.to(device)\n",
        "\n",
        "        h = net(x, torch.max(len_seq), len_seq)\n",
        "        y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "        loss = -torch.mean(t*torch_log(y) + (1 - t)*torch_log(1 - y))\n",
        "\n",
        "        pred = y.round().squeeze()\n",
        "\n",
        "        t_valid.extend(t.tolist())\n",
        "        y_pred.extend(pred.tolist())\n",
        "\n",
        "        losses_valid.append(loss.tolist())\n",
        "\n",
        "    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n",
        "        epoch,\n",
        "        np.mean(losses_train),\n",
        "        np.mean(losses_valid),\n",
        "        f1_score(t_valid, y_pred, average='macro')\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dQ8xtUcHj6IR"
      },
      "outputs": [],
      "source": [
        "net.eval()\n",
        "\n",
        "y_pred = []\n",
        "for _, line, len_seq in test_dataloader:\n",
        "\n",
        "    x = line.to(device)\n",
        "    len_seq.to(device)\n",
        "\n",
        "    h = net(x, torch.max(len_seq), len_seq)\n",
        "    y = torch.sigmoid(h).squeeze()\n",
        "\n",
        "    pred = y.round().squeeze()  # 0.5以上の値を持つ要素を正ラベルと予測する\n",
        "\n",
        "    y_pred.extend(pred.tolist())\n",
        "\n",
        "\n",
        "submission = pd.Series(y_pred, name='label')\n",
        "submission.to_csv(work_dir + '/Lecture07/submission_pred.csv', header=True, index_label='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-MRCNboHZNd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}